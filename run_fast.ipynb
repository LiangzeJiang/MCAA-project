{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy.random as random\n",
    "import time\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10.0, 6.0]\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For competition, You should be able to read and write CSV files. For example, in Python, you may use numpy.loadtxt() and numpy.savetxt(). You may also require analogs of functions such as numpy.reshape() and matplotlib.pyplot.imshow()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(num_samples, dim, sigma, scaling=1, seed=0):\n",
    "    random.seed(seed)\n",
    "    theta_gt = random.randint(0, 2, size=(dim,))\n",
    "    x = random.randn(num_samples,dim) * scaling\n",
    "    noise = random.randn(num_samples) * sigma\n",
    "    y = x @ theta_gt + noise\n",
    "    return x, y, theta_gt\n",
    "\n",
    "def state_init(dim, sparsity=None, func=\"no_sparse_constraint\"):\n",
    "    if func == \"no_sparse_constraint\":\n",
    "        init_theta = random.randint(0, 2, size=(dim,))\n",
    "        return init_theta\n",
    "    elif func == \"sparse_constraint\":\n",
    "        assert sparsity is not None\n",
    "        init_theta = np.zeros(dim)\n",
    "        indices_of_ones = random.choice(dim, sparsity, replace=False)\n",
    "        init_theta[indices_of_ones] = 1\n",
    "        return init_theta\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def trans(x, curr_pred, theta, func=\"1_bit_random_flip\", sparsity=None, indices_one=None, indices_zero=None):\n",
    "    new_theta = np.copy(theta)\n",
    "    others_to_return = None\n",
    "    if func == \"1_bit_random_flip\":  # question 1\n",
    "        ind = random.randint(0, len(theta))\n",
    "        new_theta[ind] = 1 - theta[ind]\n",
    "        diff = x[:, ind] * (1 - 2 * theta[ind])\n",
    "    elif func == \"multi_bit_random_flip_for_sparsity\":  # question 2 & 3\n",
    "        assert indices_one is not None and indices_zero is not None\n",
    "        assert sparsity is not None\n",
    "        num = random.randint(1, 4)\n",
    "        loc_one = random.choice(len(indices_one), num, replace=False)\n",
    "        loc_zero = random.choice(len(indices_zero), num, replace=False)\n",
    "        indice_one = indices_one[loc_one]\n",
    "        indice_zero = indices_zero[loc_zero]\n",
    "        new_theta[indice_one] = 0\n",
    "        new_theta[indice_zero] = 1\n",
    "        # delete old and add new indices\n",
    "        indices_one = np.delete(indices_one, loc_one)\n",
    "        indices_zero= np.delete(indices_zero, loc_zero)\n",
    "\n",
    "        indices_one = np.append(indices_one, indice_zero)\n",
    "        indices_zero = np.append(indices_zero, indice_one)\n",
    "        diff = x[:, indice_zero].sum(axis=1) - x[:, indice_one].sum(axis=1)\n",
    "        others_to_return = (indices_one, indices_zero)\n",
    "    elif func == \"2_bit_random_flip_for_sparsity\":  # question 2 & 3\n",
    "        assert indices_one is not None and indices_zero is not None\n",
    "        assert sparsity is not None\n",
    "        loc_one = random.randint(0, len(indices_one))\n",
    "        loc_zero = random.randint(0, len(indices_zero))\n",
    "        indice_one = indices_one[loc_one]\n",
    "        indice_zero = indices_zero[loc_zero]\n",
    "        new_theta[indice_one] = 0\n",
    "        new_theta[indice_zero] = 1\n",
    "        # delete old and add new indices\n",
    "        indices_one = np.delete(indices_one, loc_one)\n",
    "        indices_zero= np.delete(indices_zero, loc_zero)\n",
    "\n",
    "        indices_one = np.append(indices_one, indice_zero)\n",
    "        indices_zero = np.append(indices_zero, indice_one)\n",
    "        diff = x[:, indice_zero] - x[:, indice_one]\n",
    "        others_to_return = (indices_one, indices_zero)\n",
    "    elif func == \"2_bit_random_swap_for_sparsity\":  # question 2 & 3, mentioned in the project description\n",
    "        assert sparsity is not None\n",
    "        indices = random.choice(len(theta), 2, replace=False)\n",
    "        if theta[indices[0]] == theta[indices[1]]:\n",
    "            diff = 0\n",
    "        else:\n",
    "            new_theta[indices[0]], new_theta[indices[1]] = new_theta[indices[1]], new_theta[indices[0]]\n",
    "            diff = x[:, indices[0]] * (1 - 2 * theta[indices[0]]) + x[:, indices[1]] * (1 - 2 * theta[indices[1]])\n",
    "    elif func == \"2_bit_random_swap_for_sparsity_improved\":  # question 2 & 3, too slow\n",
    "        assert sparsity is not None\n",
    "        indices = random.choice(len(theta), 2, replace=False)\n",
    "        while theta[indices[0]] == theta[indices[1]]:\n",
    "            indices = random.choice(len(theta), 2, replace=False)\n",
    "        new_theta[indices[0]], new_theta[indices[1]] = new_theta[indices[1]], new_theta[indices[0]]\n",
    "        diff = x[:, indices[0]] * (1 - 2 * theta[indices[0]]) + x[:, indices[1]] * (1 - 2 * theta[indices[1]])\n",
    "    elif func == \"random_walk_for_sparsity\":  # question 2 & 3, not working\n",
    "        assert sparsity is not None\n",
    "        new_theta = np.zeros(len(theta))\n",
    "        indices_of_ones = random.choice(len(theta), sparsity, replace=False)\n",
    "        new_theta[indices_of_ones] = 1\n",
    "        diff = x[:, indices_of_ones].sum(axis=1) - curr_pred\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return new_theta, curr_pred + diff, others_to_return\n",
    "\n",
    "def obj(y, pred, func=\"2-norm\", delta=0.01):\n",
    "    if func == \"2-norm\":\n",
    "        return np.linalg.norm(y - pred)\n",
    "    elif func == \"1-norm\":\n",
    "        return np.linalg.norm(y - pred, ord=1)\n",
    "    elif func == \"test\":  # to check if the obj is the bottleneck. The answer was yes.\n",
    "        return 1\n",
    "    elif func == \"logistic_exponential\":\n",
    "        y = (y + 1) / 2\n",
    "        pred = (pred + 1) / 2\n",
    "        return np.log(1 + np.exp(-y * pred)).mean()\n",
    "    elif func == \"zero_one_loss\":\n",
    "        return (y * pred < 0).mean()\n",
    "    elif func == \"hinge_loss\":\n",
    "        return np.maximum(0, 1 - y * pred).mean()\n",
    "    elif func == \"bce_loss\":\n",
    "        y = (y + 1) / 2\n",
    "        pred = (pred + 1) / 2\n",
    "        return - (y * np.log(pred + delta) + (1 - y) * np.log(1 - pred + delta)).mean()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def accept(beta, diff):\n",
    "    if beta * diff <= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.exp(-beta * diff)\n",
    "\n",
    "def opt_beta_estimator(x, y, obj, obj_func, eps, dim, num_trials=10000):\n",
    "    vals = []\n",
    "    for _ in range(num_trials):\n",
    "        theta = np.bool_(random.randint(0, 2, size=(dim,)))\n",
    "        pred = x[:, theta].sum(axis=1)\n",
    "        vals.append(obj(y, pred, obj_func))\n",
    "    sorted_vals = np.sort(vals)\n",
    "    f0 = sorted_vals[0]\n",
    "    N0 = ((sorted_vals - f0) == 0).sum()\n",
    "    f1_index = np.where((sorted_vals - f0) > 0)[0][0]\n",
    "    f1 = sorted_vals[f1_index]\n",
    "    N1 = ((sorted_vals - f1) == 0).sum()\n",
    "    estimated_beta = np.log(N1 / (N0 * eps)) / (f1 - f0)\n",
    "    return estimated_beta\n",
    "\n",
    "def gen_beta_seq(x, y, obj, eps, dim, num_iters=1000, func=\"constant\", obj_func=\"2-norm\", constant=1):\n",
    "    if func == \"constant\":\n",
    "        betas = np.ones((num_iters,)) * constant\n",
    "    elif func == \"constant_opt\":\n",
    "        opt_beta = opt_beta_estimator(x, y, obj, obj_func, eps, dim)\n",
    "        print(\"estimated beta: \", opt_beta)\n",
    "        betas = np.ones((num_iters,)) * opt_beta\n",
    "    elif func == \"simulated_annealing\":\n",
    "        betas = np.linspace(0.5, 3, num_iters)\n",
    "        # betas = np.geomspace(0.1, 5, num_iters)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return betas\n",
    "\n",
    "def eval(theta_gt, thetas, coef, warmup_iters, mode='normal'):\n",
    "    if warmup_iters < 0:\n",
    "        theta = thetas[-1]\n",
    "    else:\n",
    "        if mode == 'normal':\n",
    "            theta = np.mean(thetas[warmup_iters:], axis=0)\n",
    "        elif mode == 'hard':\n",
    "            theta = np.mean(thetas[warmup_iters:], axis=0)\n",
    "            theta = np.round(theta)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    return np.linalg.norm(theta_gt - theta) * coef\n",
    "\n",
    "def run_MH(x, y, num_iters, betas, trans_func, obj_func, curr_theta, sparsity=None):\n",
    "    thetas, vals = [], []\n",
    "    curr_pred = x @ curr_theta\n",
    "    curr_val = obj(y, curr_pred, obj_func)\n",
    "\n",
    "    if trans_func == \"2_bit_random_flip_for_sparsity\" or trans_func == \"multi_bit_random_flip_for_sparsity\":  # avoid np.where every time\n",
    "        indices_one = np.where(curr_theta==1)[0]\n",
    "        indices_zero = np.where(curr_theta==0)[0]\n",
    "    else:\n",
    "        indices_one = None\n",
    "        indices_zero = None\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        next_theta, next_pred, others = trans(x, curr_pred, curr_theta, trans_func, sparsity=sparsity, indices_one=indices_one, indices_zero=indices_zero)\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        next_val = obj(y, next_pred, obj_func)\n",
    "        acceptance = accept(betas[i], next_val - curr_val)\n",
    "        # print(acceptance)\n",
    "        if random.random() <= acceptance:\n",
    "            curr_theta = next_theta\n",
    "            curr_pred = next_pred\n",
    "            curr_val = next_val\n",
    "        thetas.append(curr_theta)\n",
    "        vals.append(curr_val)\n",
    "\n",
    "        if trans_func == \"2_bit_random_flip_for_sparsity\" or trans_func == \"multi_bit_random_flip_for_sparsity\":  # avoid np.where every time\n",
    "            indices_one, indices_zero = others\n",
    "\n",
    "    return thetas, vals\n",
    "\n",
    "def plot_error(num_samples_list, error_list, coef='d'):\n",
    "    # plot the means squraed error as a function of the number of samples\n",
    "    plt.plot(num_samples_list, error_list, marker='o')\n",
    "    plt.grid()\n",
    "    if coef=='d':\n",
    "        plt.title(r\"Mean squared error ($\\frac{2}{d}\\mathbb{E} \\Vert \\hat{\\theta}-\\theta \\Vert ^2$) as a function of the number of samples\")\n",
    "    else:\n",
    "        plt.title(r\"Mean squared error ($\\frac{1}{2s}\\mathbb{E} \\Vert \\hat{\\theta}-\\theta \\Vert ^2$) as a function of the number of samples\")\n",
    "    plt.xlabel(\"number of samples\")\n",
    "    plt.ylabel(\"mean squared error\")\n",
    "\n",
    "def plot_error_wrt_beta(num_samples_list, error_list, beta_list, coef='d'):\n",
    "    # plot the means squraed error as a function of the number of samples\n",
    "    marker_list = ['o', 'v', 's', 'x', '*', '>', '<']\n",
    "    for i, (b, e) in enumerate(error_list.items()):\n",
    "        plt.plot(num_samples_list, e, marker=marker_list[i], label=r'$\\beta$='+str(b))\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"number of samples\")\n",
    "    plt.ylabel(\"mean squared error\")\n",
    "\n",
    "def plot_obj(num_samples_list, all_vals):\n",
    "    # plot the objective as a function of the number of iterations\n",
    "    for i, vals in enumerate(all_vals):\n",
    "        plt.plot(range(len(vals)), vals)\n",
    "    plt.grid()\n",
    "    plt.legend(num_samples_list)\n",
    "    plt.title(r\"Objective as a function of the number of iterations\")  # remember to change the coef\n",
    "    plt.xlabel(\"number of iterations\")\n",
    "    plt.ylabel(\"Objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples =[100, 1000, 2000, 3000, 4000, 5000, 6000]\n",
    "dim=2000 # [2000, 5000]\n",
    "sigma = 1 # noise\n",
    "num_iters = 100 * dim\n",
    "warmup_iters = 70 * dim # assume convergence after warmup\n",
    "seeds = range(3)\n",
    "eps = 0.2  # for beta estimation\n",
    "beta_list = [0.1, 1, 5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First run with fixed beta=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func = \"2-norm\"\n",
    "trans_func = \"1_bit_random_flip\"\n",
    "init_func = \"no_sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "coef = 2/dim\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=-1)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func = \"2-norm\"\n",
    "trans_func = \"1_bit_random_flip\"\n",
    "init_func = \"no_sparse_constraint\"\n",
    "beta_func = \"simulated_annealing\"\n",
    "coef = 2/dim\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "all_errors, all_vals= [], []\n",
    "for m in num_samples:\n",
    "    error, vals = 0, np.zeros((num_iters,))\n",
    "    for s in seeds:\n",
    "        print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "        x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "        betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "        init_theta = state_init(dim, init_func)\n",
    "        theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta)\n",
    "\n",
    "        error += eval(theta_gt, theta_seq, coef, warmup_iters=-1)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "        vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "    all_errors.append(error)\n",
    "    all_vals.append(vals)\n",
    "\n",
    "all_errors_wrt_beta['Simulated Annealing'] = all_errors\n",
    "\n",
    "# plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "we start with the transition function that is mentioned in the project description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 1, we change transition function, initialization function and the coefficient\n",
    "obj_func = \"2-norm\"\n",
    "trans_func = \"2_bit_random_swap_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/100) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=warmup_iters)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not converged, is it because of sparsity?\n",
    "It might because the transition prob for self-loop is too high in the above case, especially when the theta is sparse. Below we show that reducing sparsity improves the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 1, we change transition function, initialization function and the coefficient\n",
    "obj_func = \"2-norm\"\n",
    "trans_func = \"2_bit_random_swap_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/5) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=-1)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about without self-loop?\n",
    "This one removes the self-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 1, we change transition function, initialization function and the coefficient\n",
    "obj_func = \"2-norm\"\n",
    "trans_func = \"2_bit_random_flip_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/100) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=warmup_iters)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.mean(theta_seq[warmup_iters:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 1, we change transition function, initialization function and the coefficient\n",
    "obj_func = \"2-norm\"\n",
    "trans_func = \"2_bit_random_flip_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/100) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "seeds = [0]\n",
    "num_samples = [5, 10, 100]\n",
    "beta_list = [0.5, 1, 10]\n",
    "warmup_iters = 70 * dim # assume convergence after warmup\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=warmup_iters, mode='normal')/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about random walk in a clique?\n",
    "There is self-loop (since it's a random walk), but the transition prob is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 1, we change transition function, initialization function and the coefficient\n",
    "obj_func = \"2-norm\"\n",
    "trans_func = \"random_walk_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/100) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=-1)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 2, we change objective function (TODO)\n",
    "obj_func = \"logistic_exponential\"  # logistic_exponential, zero_one_loss, hinge_loss, bce_loss\n",
    "trans_func = \"2_bit_random_flip_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/100) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=-1)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 2, we change objective function (TODO)\n",
    "obj_func = \"zero_one_loss\"  # logistic_exponential, zero_one_loss, hinge_loss, bce_loss\n",
    "trans_func = \"2_bit_random_flip_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/100) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=-1)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 2, we change objective function (TODO)\n",
    "obj_func = \"hinge_loss\"  # logistic_exponential, zero_one_loss, hinge_loss, bce_loss\n",
    "trans_func = \"2_bit_random_flip_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/100) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=-1)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to question 2, we change objective function (TODO)\n",
    "obj_func = \"logistic_exponential\"  # logistic_exponential, zero_one_loss, hinge_loss, bce_loss\n",
    "trans_func = \"2_bit_random_flip_for_sparsity\"\n",
    "init_func = \"sparse_constraint\"\n",
    "beta_func = \"constant\"\n",
    "sparsity = int(dim/100) # only used for question 2 & 3\n",
    "coef = 1/(2 * sparsity)\n",
    "\n",
    "all_errors_wrt_beta = {}\n",
    "for b in beta_list:\n",
    "    all_errors, all_vals= [], []\n",
    "    for m in num_samples:\n",
    "        error, vals = 0, np.zeros((num_iters,))\n",
    "        for s in seeds:\n",
    "            print(f\"Running beta: {b}, num_samples: {m}, seed:{s}...\")\n",
    "            x, y, theta_gt = gen_data(m, dim, sigma, seed=s)\n",
    "            betas = gen_beta_seq(x, y, obj, eps, dim, num_iters, beta_func, constant=b)\n",
    "            init_theta = state_init(dim, sparsity=sparsity, func=init_func)\n",
    "            theta_seq, val_seq = run_MH(x, y, num_iters, betas, trans_func, obj_func, init_theta, sparsity=sparsity)\n",
    "\n",
    "            error += eval(theta_gt, theta_seq, coef, warmup_iters=-1)/len(seeds) # estimate mean squared error, averaged over theta and X\n",
    "            vals += np.array(val_seq)/len(seeds)\n",
    "\n",
    "        all_errors.append(error)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_errors_wrt_beta[b] = all_errors\n",
    "\n",
    "    # plot_error(num_samples, all_errors)\n",
    "\n",
    "plot_error_wrt_beta(num_samples, all_errors_wrt_beta, beta_list, coef='s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-hume1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
